"""Mail Data Refresher for automatic database updates"""

import asyncio
import logging
import os
import sys
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Any, Optional, List

# Add parent directory to path
sys.path.append(str(Path(__file__).parent.parent.parent))

from infra.core.database import get_database_manager
from modules.account import AccountOrchestrator
from modules.mail_query import MailQueryOrchestrator, MailQueryRequest
from modules.mail_process import MailProcessorOrchestrator
from modules.mail_dashboard import EmailDashboardOrchestrator
from infra.core.kafka_client import get_kafka_client

logger = logging.getLogger(__name__)


class MailDataRefresher:
    """ìë™ìœ¼ë¡œ ë©”ì¼ ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ê³  SQLite DBì— ì €ì¥í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.db_manager = get_database_manager()
        self.account_orchestrator = AccountOrchestrator()
        self.mail_query_orchestrator = MailQueryOrchestrator()
        self.mail_processor = MailProcessorOrchestrator()
        self.dashboard_orchestrator = EmailDashboardOrchestrator()
        self.kafka_client = get_kafka_client()
        logger.info("ğŸ“§ MailDataRefresher initialized")
        
    async def get_last_mail_date(self, user_id: str) -> Optional[datetime]:
        """agenda_allì—ì„œ ë§ˆì§€ë§‰ ë©”ì¼ ìˆ˜ì‹  ì‹œê°„ ì¡°íšŒ"""
        try:
            query = """
            SELECT MAX(sent_time) as last_received
            FROM agenda_all
            """
            
            # Use synchronous database call - no parameters needed
            result = self.db_manager.fetch_one(query)
            
            if result and result[0]:
                # ISO format string to datetime
                if isinstance(result[0], str):
                    return datetime.fromisoformat(result[0].replace('Z', '+00:00'))
                return result[0]
            
            # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ 30ì¼ ì „ë¶€í„°
            return datetime.now() - timedelta(days=30)
            
        except Exception as e:
            logger.error(f"Error getting last mail date: {e}")
            return datetime.now() - timedelta(days=30)
    
    async def refresh_mail_data_for_user(
        self, 
        user_id: str = "krsdtp",
        max_mails: int = 1000,
        use_last_date: bool = True
    ) -> Dict[str, Any]:
        """íŠ¹ì • ì‚¬ìš©ìì˜ ë©”ì¼ ë°ì´í„° ìµœì‹ í™”"""
        logger.info(f"ğŸ”„ Starting mail data refresh for user: {user_id}")
        
        try:
            # 1. ê³„ì • ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            account = self.account_orchestrator.account_get_by_user_id(user_id)
            if not account:
                raise ValueError(f"Account not found for user_id: {user_id}")
            
            if not account.is_active:
                raise ValueError(f"Account is not active: {user_id}")
            
            # 2. ì‹œì‘ ë‚ ì§œ ê²°ì •
            if use_last_date:
                start_date = await self.get_last_mail_date(user_id)
                if start_date:
                    logger.info(f"ğŸ“… Using last mail date from agenda_all: {start_date}")
                else:
                    logger.info(f"ğŸ“… No previous mail found, using 30 days back")
                    start_date = datetime.now() - timedelta(days=30)
            else:
                start_date = datetime.now() - timedelta(days=30)
                
            # ë‚ ì§œ ê³„ì‚° - timezone aware ì²˜ë¦¬
            from datetime import timezone
            
            # í•­ìƒ timezone aware datetime ì‚¬ìš©
            if start_date.tzinfo is None:
                start_date = start_date.replace(tzinfo=timezone.utc)
            
            # í˜„ì¬ ì‹œê°„ë„ UTCë¡œ
            now = datetime.now(timezone.utc)
                
            days_back = (now - start_date).days
            if days_back < 1:
                days_back = 1
                
            logger.info(f"ğŸ“Š Query parameters: days_back={days_back}, max_mails={max_mails}")
            
            # 3. ë©”ì¼ ì¡°íšŒ - ë‚ ì§œ í•„í„° ì¶”ê°€
            from modules.mail_query.mail_query_schema import MailQueryFilters, PaginationOptions
            
            # ë‚ ì§œ í•„í„° ìƒì„±
            filters = MailQueryFilters(
                date_from=start_date,  # ë§ˆì§€ë§‰ ìˆ˜ì‹  ë‚ ì§œë¶€í„°
                date_to=now  # í˜„ì¬ê¹Œì§€
            )
            
            # í˜ì´ì§• ì˜µì…˜
            pagination = PaginationOptions(
                top=50,  # í•œ ë²ˆì— 50ê°œì”©
                max_pages=max_mails // 50 if max_mails > 50 else 1  # ìµœëŒ€ í˜ì´ì§€ ìˆ˜ ê³„ì‚°
            )
            
            query_request = MailQueryRequest(
                user_id=user_id,
                filters=filters,
                pagination=pagination
            )
            
            logger.info(f"ğŸ” Querying emails for {user_id}...")
            logger.info(f"ğŸ“… Date filter: {start_date.strftime('%Y-%m-%d %H:%M:%S')} to {now.strftime('%Y-%m-%d %H:%M:%S')}")
            logger.info(f"ğŸ“Š Filter object: date_from={filters.date_from}, date_to={filters.date_to}")
            logger.info(f"ğŸ“„ Pagination: top={pagination.top}, max_pages={pagination.max_pages}")
            query_response = await self.mail_query_orchestrator.mail_query_user_emails(query_request)
            
            mail_count = len(query_response.messages) if query_response.messages else 0
            logger.info(f"ğŸ“¬ Found {mail_count} emails")
            
            # 4. ë©”ì¼ ì²˜ë¦¬ ë° DB ì €ì¥
            processed_count = 0
            if query_response.messages:
                logger.info(f"ğŸ’¾ Processing and saving emails to database...")
                
                # ë°°ì¹˜ë¡œ ì²˜ë¦¬
                await self.mail_processor.enqueue_mail_batch(
                    account.id, 
                    query_response.messages
                )
                
                # ì²˜ë¦¬ ì‹¤í–‰
                process_result = await self.mail_processor.process_batch()
                
                # process_resultê°€ dictì¸ì§€ í™•ì¸
                if isinstance(process_result, dict):
                    processed_count = process_result.get('processed_count', 0)
                elif isinstance(process_result, list):
                    processed_count = len(process_result)
                else:
                    processed_count = 0
                
                logger.info(f"âœ… Processed {processed_count} emails")
            
            # 5. ì´ë²¤íŠ¸ ìˆ˜ì§‘ ë° agenda_chair ì €ì¥
            events_processed = 0
            if processed_count > 0:
                logger.info(f"ğŸ“¨ Collecting events from Kafka and saving to agenda_chair...")
                events_result = await self.collect_and_save_events(
                    max_events=processed_count * 2,  # ì—¬ìœ ìˆê²Œ ì„¤ì •
                    timeout_seconds=30
                )
                events_processed = events_result.get('processed_count', 0)
                logger.info(f"âœ… Processed {events_processed} events to agenda_chair")
            
            return {
                "status": "success",
                "user_id": user_id,
                "start_date": start_date.isoformat(),
                "end_date": datetime.now().isoformat(),
                "days_back": days_back,
                "mail_count": mail_count,
                "processed_count": processed_count,
                "events_processed": events_processed,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"âŒ Error refreshing mail data for {user_id}: {e}")
            return {
                "status": "failed",
                "user_id": user_id,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
    
    async def collect_and_save_events(
        self, 
        max_events: int = 1000,
        timeout_seconds: int = 30,
        topic: str = "email.received",
        consumer_group: str = "mcp-mail-refresher"
    ) -> Dict[str, Any]:
        """Kafkaì—ì„œ ì´ë²¤íŠ¸ë¥¼ ìˆ˜ì§‘í•˜ì—¬ agenda_chair í…Œì´ë¸”ì— ì €ì¥"""
        try:
            import json
            from kafka import KafkaConsumer
            
            logger.info(f"ğŸ”„ Starting event collection from Kafka topic: {topic}")
            
            # Kafka consumer ì„¤ì •
            consumer_config = {
                "bootstrap_servers": "localhost:9092",
                "group_id": consumer_group,
                "auto_offset_reset": "earliest",
                "enable_auto_commit": True,
                "consumer_timeout_ms": timeout_seconds * 1000,
                "max_poll_records": max_events,
                "value_deserializer": lambda x: (
                    json.loads(x.decode("utf-8")) if x else None
                ),
            }
            
            consumer = KafkaConsumer(topic, **consumer_config)
            
            collected_events = []
            processed_count = 0
            chair_events = 0
            member_events = 0
            
            # ì´ë²¤íŠ¸ ìˆ˜ì§‘
            logger.info(f"ğŸ“¥ Collecting up to {max_events} events...")
            for message in consumer:
                try:
                    event_data = message.value
                    if event_data:
                        collected_events.append(event_data)
                        
                        # ì´ë²¤íŠ¸ íƒ€ì… í™•ì¸
                        event_info = event_data.get("event_info", {})
                        sender_type = str(event_info.get("sender_type", "")).upper()
                        
                        if sender_type == "CHAIR":
                            chair_events += 1
                        elif sender_type == "MEMBER":
                            member_events += 1
                        
                        if len(collected_events) >= max_events:
                            break
                            
                except Exception as e:
                    logger.error(f"Error processing message: {e}")
                    continue
            
            consumer.close()
            
            logger.info(f"ğŸ“Š Collected {len(collected_events)} events (Chair: {chair_events}, Member: {member_events})")
            
            # ì´ë²¤íŠ¸ ì²˜ë¦¬ ë° ì €ì¥
            if collected_events:
                logger.info(f"ğŸ’¾ Processing events and saving to dashboard tables...")
                
                for event in collected_events:
                    try:
                        # EmailDashboardOrchestratorë¥¼ í†µí•´ ì´ë²¤íŠ¸ ì²˜ë¦¬
                        result = self.dashboard_orchestrator.handle_email_event(event)
                        
                        if result.get("success"):
                            processed_count += 1
                        else:
                            logger.warning(f"Failed to process event: {result.get('message')}")
                            
                    except Exception as e:
                        logger.error(f"Error handling event: {e}")
                        continue
                
                logger.info(f"âœ… Successfully processed {processed_count} events")
            
            return {
                "status": "success",
                "collected_count": len(collected_events),
                "processed_count": processed_count,
                "chair_events": chair_events,
                "member_events": member_events,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"âŒ Error collecting events: {e}")
            return {
                "status": "failed",
                "error": str(e),
                "collected_count": 0,
                "processed_count": 0,
                "timestamp": datetime.now().isoformat()
            }
    
    async def refresh_all_active_accounts(
        self,
        max_mails_per_account: int = 100
    ) -> Dict[str, Any]:
        """ëª¨ë“  í™œì„± ê³„ì •ì˜ ë©”ì¼ ë°ì´í„° ìµœì‹ í™”"""
        logger.info("ğŸ”„ Starting mail data refresh for all active accounts")
        
        try:
            # í™œì„± ê³„ì • ëª©ë¡ ê°€ì ¸ì˜¤ê¸° - ì§ì ‘ DB ì¿¼ë¦¬
            query = """
            SELECT id, user_id, user_name, email, is_active
            FROM accounts 
            WHERE is_active = 1
            ORDER BY user_id
            """
            
            conn = self.db_manager._get_connection()
            cursor = conn.cursor()
            cursor.execute(query)
            rows = cursor.fetchall()
            conn.close()
                
            accounts = []
            for row in rows:
                accounts.append({
                    'id': row[0],
                    'user_id': row[1],
                    'user_name': row[2],
                    'email': row[3],
                    'is_active': row[4]
                })
            
            results = []
            total_mails = 0
            total_processed = 0
            
            for account in accounts:
                result = await self.refresh_mail_data_for_user(
                    user_id=account['user_id'],
                    max_mails=max_mails_per_account,
                    use_last_date=True
                )
                
                results.append(result)
                
                if result['status'] == 'success':
                    total_mails += result.get('mail_count', 0)
                    total_processed += result.get('processed_count', 0)
            
            return {
                "status": "completed",
                "total_accounts": len(accounts),
                "total_mails": total_mails,
                "total_processed": total_processed,
                "results": results,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"âŒ Error refreshing all accounts: {e}")
            return {
                "status": "failed",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }


# Test function
async def test_refresh():
    """Test mail data refresh"""
    refresher = MailDataRefresher()
    result = await refresher.refresh_mail_data_for_user("krsdtp", max_mails=10)
    print(f"Test result: {result}")


if __name__ == "__main__":
    # Configure logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # Run test
    asyncio.run(test_refresh())