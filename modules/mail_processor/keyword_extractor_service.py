"""OpenRouterë¥¼ í™œìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ ì„œë¹„ìŠ¤"""
import re
import time
import json
import aiohttp
from collections import Counter
from typing import List, Optional

from infra.core.config import get_config
from infra.core.logger import get_logger
from .mail_processor_schema import KeywordExtractionRequest, KeywordExtractionResponse


class MailProcessorKeywordExtractorService:
    """OpenRouterë¥¼ í™œìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.config = get_config()
        self.logger = get_logger(__name__)
        
        # OpenRouter ì„¤ì •
        self.api_key = getattr(self.config, 'openrouter_api_key', None)
        self.model = "openai/gpt-3.5-turbo"  # ì§ì ‘ ì„¤ì •ìœ¼ë¡œ o3-mini ë¬¸ì œ í•´ê²°
        self.base_url = "https://openrouter.ai/api/v1"
        
    async def extract_keywords(self, text: str, max_keywords: int = 5) -> KeywordExtractionResponse:
        """ë©”ì¼ ë³¸ë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        start_time = time.time()
        
        try:
            # í…ìŠ¤íŠ¸ ì •ì œ
            clean_text = self._clean_text(text)
            
            # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ëŠ” ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            if len(clean_text.strip()) < 10:
                return KeywordExtractionResponse(
                    keywords=[],
                    method="empty_text",
                    model=self.model,
                    execution_time_ms=int((time.time() - start_time) * 1000),
                    token_info={}
                )
            
            # OpenRouter API í˜¸ì¶œ
            if self.api_key:
                keywords, token_info = await self._call_openrouter_api(clean_text, max_keywords)
                if keywords:
                    self.logger.debug(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì„±ê³µ: {keywords}")
                    return KeywordExtractionResponse(
                        keywords=keywords,
                        method="openrouter",
                        model=self.model,
                        execution_time_ms=int((time.time() - start_time) * 1000),
                        token_info=token_info
                    )
            
            # Fallback í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = self._fallback_keyword_extraction(clean_text, max_keywords)
            return KeywordExtractionResponse(
                keywords=keywords,
                method="fallback",
                model="rule_based",
                execution_time_ms=int((time.time() - start_time) * 1000),
                token_info={}
            )
                
        except Exception as e:
            self.logger.warning(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨, fallback ì‚¬ìš©: {str(e)}")
            keywords = self._fallback_keyword_extraction(text, max_keywords)
            return KeywordExtractionResponse(
                keywords=keywords,
                method="fallback_error",
                model="rule_based",
                execution_time_ms=int((time.time() - start_time) * 1000),
                token_info={}
            )
    
    async def _call_openrouter_api(self, text: str, max_keywords: int) -> tuple[List[str], dict]:
        """OpenRouter API í˜¸ì¶œ"""
        
        # API í‚¤ ìƒíƒœ í™•ì¸
        if not self.api_key:
            self.logger.error("âŒ OpenRouter API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
            return [], {}
        
        # API í‚¤ ì¼ë¶€ í‘œì‹œ (ë””ë²„ê¹…ìš©)
        self.logger.info(f"ğŸ”‘ OpenRouter API í‚¤: {self.api_key[:10]}...{self.api_key[-4:]}")
        self.logger.info(f"ğŸ“¡ OpenRouter ëª¨ë¸: {self.model}")
        self.logger.info(f"ğŸŒ OpenRouter URL: {self.base_url}/chat/completions")
        
        # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (1000ìë¡œ ì¤„ì„)
        limited_text = text[:1000] if len(text) > 1000 else text
        
        # ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸
        prompt = f"Extract {max_keywords} keywords from this email: {limited_text}\n\nKeywords:"

        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }
        
        # OpenRouter ê³µì‹ í˜•ì‹ - gpt-3.5-turboëŠ” ë” ì•ˆì •ì 
        payload = {
            "model": self.model,
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "max_tokens": 100,
            "temperature": 0.3
        }
        
        # í† í° ì •ë³´ ì´ˆê¸°í™”
        token_info = {
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0,
            "cost_usd": 0.0
        }
        
        self.logger.info(f"ğŸ“¤ ìš”ì²­ í˜ì´ë¡œë“œ: {json.dumps(payload, ensure_ascii=False)[:200]}...")
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.base_url}/chat/completions",
                    headers=headers,
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as response:
                    
                    # ì‘ë‹µ ìƒíƒœ ë° í—¤ë” ë¡œê·¸
                    self.logger.info(f"ğŸ“¥ ì‘ë‹µ ìƒíƒœ: {response.status}")
                    self.logger.info(f"ğŸ“‹ ì‘ë‹µ í—¤ë”: {dict(response.headers)}")
                    
                    # ì›ì‹œ ì‘ë‹µ í…ìŠ¤íŠ¸ ë¨¼ì € ì½ê¸°
                    response_text = await response.text()
                    self.logger.info(f"ğŸ“„ ì›ì‹œ ì‘ë‹µ: {response_text[:500]}...")
                    
                    if response.status != 200:
                        self.logger.error(f"âŒ API ì˜¤ë¥˜: {response_text}")
                        return [], token_info
                    
                    # JSON íŒŒì‹±
                    try:
                        data = json.loads(response_text)
                    except json.JSONDecodeError as e:
                        self.logger.error(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                        return [], token_info
                    
                    # ì „ì²´ ì‘ë‹µ êµ¬ì¡° ë¡œê·¸
                    self.logger.info(f"ğŸ“Š ì „ì²´ ì‘ë‹µ êµ¬ì¡°: {json.dumps(data, ensure_ascii=False, indent=2)}")
                    
                    # í† í° ì‚¬ìš©ëŸ‰ ì •ë³´ ì¶”ì¶œ
                    if 'usage' in data:
                        usage = data['usage']
                        token_info.update({
                            "prompt_tokens": usage.get('prompt_tokens', 0),
                            "completion_tokens": usage.get('completion_tokens', 0),
                            "total_tokens": usage.get('total_tokens', 0)
                        })
                        
                        # ë¹„ìš© ê³„ì‚° (gpt-3.5-turbo ê¸°ì¤€: $0.0015/1K input, $0.002/1K output)
                        input_cost = (token_info["prompt_tokens"] / 1000) * 0.0015
                        output_cost = (token_info["completion_tokens"] / 1000) * 0.002
                        token_info["cost_usd"] = round(input_cost + output_cost, 6)
                    
                    # ì‘ë‹µì—ì„œ ì»¨í…ì¸  ì¶”ì¶œ
                    if 'choices' in data and data['choices']:
                        choice = data['choices'][0]
                        if 'message' in choice and 'content' in choice['message']:
                            content = choice['message']['content']
                            self.logger.info(f"âœ… ì¶”ì¶œëœ ì»¨í…ì¸ : '{content}'")
                            
                            if content and content.strip():
                                keywords = self._parse_keywords(content)
                                self.logger.info(f"ğŸ·ï¸ íŒŒì‹±ëœ í‚¤ì›Œë“œ: {keywords}")
                                return keywords[:max_keywords], token_info
                            else:
                                self.logger.warning("âš ï¸ ì»¨í…ì¸ ê°€ ë¹„ì–´ìˆìŒ")
                        else:
                            self.logger.error(f"âŒ message.content ì—†ìŒ: {choice}")
                    else:
                        self.logger.error(f"âŒ choices ì—†ìŒ: {data}")
                    
                    return [], token_info
                    
        except aiohttp.ClientError as e:
            self.logger.error(f"âŒ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {str(e)}")
            return [], token_info
        except Exception as e:
            self.logger.error(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}", exc_info=True)
            return [], token_info

    def _fallback_keyword_extraction(self, text: str, max_keywords: int) -> List[str]:
        """OpenRouter ì‹¤íŒ¨ ì‹œ ê°„ë‹¨í•œ fallback í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ê°„ë‹¨í•œ í•œêµ­ì–´ ë‹¨ì–´ ì¶”ì¶œ
        clean_text = self._clean_text(text)
        
        # í•œêµ­ì–´ ë‹¨ì–´ ì¶”ì¶œ (2ê¸€ì ì´ìƒ)
        korean_words = re.findall(r'[ê°€-í£]{2,}', clean_text)
        
        # ì˜ë¬¸ ë‹¨ì–´ ì¶”ì¶œ (3ê¸€ì ì´ìƒ)
        english_words = re.findall(r'[A-Za-z]{3,}', clean_text)
        
        # ìˆ«ì í¬í•¨ ì‹ë³„ì ì¶”ì¶œ (ì˜ˆ: EA004, REQ-123)
        identifiers = re.findall(r'[A-Z]{2,}\d+|[A-Z]+-\d+|\d{3,}', clean_text)
        
        # ëª¨ë“  ë‹¨ì–´ í•©ì¹˜ê¸°
        all_words = korean_words + english_words + identifiers
        
        # ë¹ˆë„ìˆ˜ ê¸°ë°˜ ìƒìœ„ í‚¤ì›Œë“œ ì„ íƒ
        word_counts = Counter(all_words)
        top_keywords = [word for word, count in word_counts.most_common(max_keywords)]
        
        return top_keywords
    
    def _clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ì œ"""
        if not text:
            return ""
        
        # HTML íƒœê·¸ ì œê±°
        clean = re.sub(r'<[^>]+>', '', text)
        
        # ê³¼ë„í•œ ê³µë°± ì •ë¦¬
        clean = re.sub(r'\s+', ' ', clean)
        
        # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬ (í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê¸°ë³¸ êµ¬ë‘ì ë§Œ ìœ ì§€)
        clean = re.sub(r'[^\w\sê°€-í£.,!?()-]', ' ', clean)
        
        return clean.strip()
    
    def _parse_keywords(self, content: str) -> List[str]:
        """ë‹¤ì–‘í•œ í˜•ì‹ì˜ í‚¤ì›Œë“œ ì‘ë‹µì„ íŒŒì‹±"""
        keywords = []
        
        self.logger.debug(f"í‚¤ì›Œë“œ íŒŒì‹± ì‹œì‘: '{content}'")
        
        # 1. ë²ˆí˜¸ ë§¤ê¹€ í˜•ì‹: "1. í‚¤ì›Œë“œ1\n2. í‚¤ì›Œë“œ2\n3. í‚¤ì›Œë“œ3"
        if re.search(r'\d+\.\s*', content):
            self.logger.debug("ë²ˆí˜¸ ë§¤ê¹€ í˜•ì‹ìœ¼ë¡œ íŒŒì‹±")
            lines = content.split('\n')
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                # "1. í‚¤ì›Œë“œ" í˜•ì‹ì—ì„œ í‚¤ì›Œë“œë§Œ ì¶”ì¶œ
                match = re.match(r'\d+\.\s*(.+)', line)
                if match:
                    keyword = match.group(1).strip()
                    if keyword:
                        keywords.append(keyword)
                        self.logger.debug(f"ë²ˆí˜¸ ë§¤ê¹€ì—ì„œ ì¶”ì¶œ: '{keyword}'")
        
        # 2. ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ í˜•ì‹: "í‚¤ì›Œë“œ1, í‚¤ì›Œë“œ2, í‚¤ì›Œë“œ3"
        elif ',' in content:
            self.logger.debug("ì½¤ë§ˆ êµ¬ë¶„ í˜•ì‹ìœ¼ë¡œ íŒŒì‹±")
            keywords = [kw.strip() for kw in content.split(',') if kw.strip()]
        
        # 3. ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„ëœ í˜•ì‹: "í‚¤ì›Œë“œ1\ní‚¤ì›Œë“œ2\ní‚¤ì›Œë“œ3"
        elif '\n' in content:
            self.logger.debug("ì¤„ë°”ê¿ˆ êµ¬ë¶„ í˜•ì‹ìœ¼ë¡œ íŒŒì‹±")
            keywords = [line.strip() for line in content.split('\n') if line.strip()]
        
        # 4. ê³µë°±ìœ¼ë¡œ êµ¬ë¶„ëœ í˜•ì‹: "í‚¤ì›Œë“œ1 í‚¤ì›Œë“œ2 í‚¤ì›Œë“œ3"
        else:
            self.logger.debug("ê³µë°± êµ¬ë¶„ í˜•ì‹ìœ¼ë¡œ íŒŒì‹±")
            keywords = content.split()
        
        # í‚¤ì›Œë“œ ì •ì œ
        cleaned_keywords = []
        for kw in keywords:
            # ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±° (ì•ë’¤ íŠ¹ìˆ˜ë¬¸ì)
            kw = re.sub(r'^[^\wê°€-í£]+|[^\wê°€-í£]+$', '', kw)
            # ìµœì†Œ ê¸¸ì´ í™•ì¸ (2ê¸€ì ì´ìƒ)
            if kw and len(kw) >= 2:
                cleaned_keywords.append(kw)
                self.logger.debug(f"ì •ì œëœ í‚¤ì›Œë“œ: '{kw}'")
        
        self.logger.debug(f"ìµœì¢… íŒŒì‹± ê²°ê³¼: {cleaned_keywords}")
        return cleaned_keywords
