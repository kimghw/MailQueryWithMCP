#!/usr/bin/env python3
"""
ëª¨ë“  ê³„ì •ì˜ ë©”ì¼ ì¡°íšŒ ë° ì²˜ë¦¬ í†µí•© í…ŒìŠ¤í„° (ì¤‘ë³µ ì²´í¬ ë¡œì§ í…ŒìŠ¤íŠ¸ í¬í•¨)
ê°œì„ ì‚¬í•­: í‚¤ì›Œë“œ ì¶”ì¶œ ëª¨ë“ˆ í†µí•©, ì—ëŸ¬ í•¸ë“¤ë§ ê°•í™”
"""
"""
# ê¸°ë³¸ ì‹¤í–‰
python mail_query_process.py

# ìµœê·¼ 30ì¼, ê³„ì •ë‹¹ 50ê°œ ë©”ì¼
python mail_query_process.py 30 50

# mail_history ì´ˆê¸°í™” í›„ ì‹¤í–‰
python mail_query_process.py --clear-history

# íŠ¹ì • ì‚¬ìš©ì ì¤‘ë³µ ì²´í¬ í…ŒìŠ¤íŠ¸
python mail_query_process.py --test-duplicate user123

"""

import sys
import os
import sqlite3

# Python ê²½ë¡œì— í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

import asyncio
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import json
from collections import defaultdict, Counter

# ì§ì ‘ import
from modules.mail_query.mail_query_orchestrator import MailQueryOrchestrator
from modules.mail_query.mail_query_schema import (
    MailQueryRequest,
    MailQueryFilters,
    PaginationOptions,
)
from modules.mail_process.mail_processor_orchestrator import MailProcessorOrchestrator

from infra.core.database import get_database_manager
from infra.core.logger import get_logger
from infra.core.config import get_config

# ë¡œê·¸ ë ˆë²¨ ì„¤ì •
update_all_loggers_level("INFO")
logger = get_logger(__name__)


class AllAccountsFullProcessTester:
    """ëª¨ë“  ê³„ì •ì˜ ë©”ì¼ ì¡°íšŒ ë° ì²˜ë¦¬ í†µí•© í…ŒìŠ¤í„°"""

    def __init__(self):
        self.mail_query = MailQueryOrchestrator()
        self.mail_processor = MailProcessorOrchestrator()
        self.db = get_database_manager()
        self.config = get_config()

        # í˜„ì¬ ì¤‘ë³µ ì²´í¬ ì„¤ì • í™•ì¸ (config í†µí•´ì„œ)
        self.duplicate_check_enabled = (
            self.config.get_setting("ENABLE_MAIL_DUPLICATE_CHECK", "true").lower()
            == "true"
        )

        # í‚¤ì›Œë“œ ì¶”ì¶œ ì„¤ì • í™•ì¸
        self.structured_extraction = (
            self.config.get_setting("ENABLE_STRUCTURED_EXTRACTION", "true").lower()
            == "true"
        )

        self.batch_extraction = (
            self.config.get_setting("ENABLE_BATCH_KEYWORD_EXTRACTION", "true").lower()
            == "true"
        )

        logger.info(
            f"ğŸ” ì¤‘ë³µ ì²´í¬: {'í™œì„±í™”' if self.duplicate_check_enabled else 'ë¹„í™œì„±í™”'}"
        )
        logger.info(
            f"ğŸ” êµ¬ì¡°í™”ëœ ì¶”ì¶œ: {'í™œì„±í™”' if self.structured_extraction else 'ë¹„í™œì„±í™”'}"
        )
        logger.info(
            f"ğŸ” ë°°ì¹˜ ì¶”ì¶œ: {'í™œì„±í™”' if self.batch_extraction else 'ë¹„í™œì„±í™”'}"
        )

    async def get_all_active_accounts(self) -> List[Dict[str, Any]]:
        """í™œì„±í™”ëœ ëª¨ë“  ê³„ì • ì¡°íšŒ (í…ŒìŠ¤íŠ¸ ê³„ì • ì œì™¸)"""
        query = """
            SELECT 
                user_id, 
                user_name, 
                email,
                is_active,
                status,
                last_sync_time
            FROM accounts 
            WHERE is_active = 1
            AND user_id NOT IN ('test_user', 'test', 'nonexistent', 'temp_user', 'demo_user')
            AND user_id NOT LIKE 'test_%'
            AND user_id NOT LIKE 'temp_%'
            ORDER BY user_id
        """

        try:
            accounts = self.db.fetch_all(query)
            return [dict(account) for account in accounts]
        except Exception as e:
            logger.error(f"ê³„ì • ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return []

    async def check_existing_mails(self, user_id: str) -> Dict[str, int]:
        """ê¸°ì¡´ ì €ì¥ëœ ë©”ì¼ í†µê³„ í™•ì¸ (ì¤‘ë³µ ì²´í¬ìš©)"""
        try:
            # user_idë¡œ ì‹¤ì œ account_id ì¡°íšŒ
            account_result = self.db.fetch_one(
                "SELECT id FROM accounts WHERE user_id = ?", (user_id,)
            )

            if not account_result:
                return {"total": 0, "recent": 0, "by_month": {}}

            account_id = account_result["id"]

            # ì „ì²´ ì €ì¥ëœ ë©”ì¼ ìˆ˜
            total_result = self.db.fetch_one(
                "SELECT COUNT(*) as count FROM mail_history WHERE account_id = ?",
                (account_id,),
            )

            # ìµœê·¼ 7ì¼ê°„ ì €ì¥ëœ ë©”ì¼ ìˆ˜
            recent_result = self.db.fetch_one(
                """
                SELECT COUNT(*) as count 
                FROM mail_history 
                WHERE account_id = ? 
                AND processed_at >= datetime('now', '-7 days')
                """,
                (account_id,),
            )

            # ì›”ë³„ í†µê³„
            monthly_stats = self.db.fetch_all(
                """
                SELECT 
                    strftime('%Y-%m', sent_time) as month,
                    COUNT(*) as count
                FROM mail_history
                WHERE account_id = ?
                GROUP BY month
                ORDER BY month DESC
                LIMIT 6
                """,
                (account_id,),
            )

            return {
                "total": total_result["count"] if total_result else 0,
                "recent": recent_result["count"] if recent_result else 0,
                "by_month": (
                    {row["month"]: row["count"] for row in monthly_stats}
                    if monthly_stats
                    else {}
                ),
            }
        except Exception as e:
            logger.error(f"ê¸°ì¡´ ë©”ì¼ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return {"total": 0, "recent": 0, "by_month": {}}

    async def process_account(
        self, user_id: str, user_name: str, days_back: int = 60, max_mails: int = 10
    ) -> Dict[str, Any]:
        """ë‹¨ì¼ ê³„ì •ì˜ ë©”ì¼ ì¡°íšŒ ë° ì²˜ë¦¬"""

        start_time = datetime.now()
        result = {
            "user_id": user_id,
            "user_name": user_name,
            "duplicate_check_enabled": self.duplicate_check_enabled,
            "structured_extraction": self.structured_extraction,
            "batch_extraction": self.batch_extraction,
            "existing_mails": {"total": 0, "recent": 0, "by_month": {}},
            "query_success": False,
            "process_success": False,
            "total_mails_found": 0,
            "mails_processed": 0,
            "processing_stats": {
                "success": 0,
                "skipped": 0,
                "failed": 0,
                "filtered": 0,
                "duplicate": 0,
                "events_published": 0,
                "keywords_extracted": 0,
            },
            "keywords": {"all": [], "unique": [], "top_10": []},
            "structured_data_samples": [],  # êµ¬ì¡°í™”ëœ ë°ì´í„° ìƒ˜í”Œ
            "execution_time": {"query_ms": 0, "process_ms": 0, "total_ms": 0},
            "errors": [],
        }

        try:
            # 0. ê¸°ì¡´ ë©”ì¼ í†µê³„ í™•ì¸ (ì¤‘ë³µ ì²´í¬ í™œì„±í™”ëœ ê²½ìš°)
            if self.duplicate_check_enabled:
                result["existing_mails"] = await self.check_existing_mails(user_id)
                logger.info(
                    f"ğŸ“Š [{user_id}] ê¸°ì¡´ ì €ì¥ëœ ë©”ì¼: "
                    f"ì „ì²´={result['existing_mails']['total']}ê°œ, "
                    f"ìµœê·¼ 7ì¼={result['existing_mails']['recent']}ê°œ"
                )

            # 1. Mail Query - ë©”ì¼ ì¡°íšŒ
            logger.info(f"\nğŸ“¥ [{user_id}] ë©”ì¼ ì¡°íšŒ ì‹œì‘...")
            query_start = datetime.now()

            request = MailQueryRequest(
                user_id=user_id,
                filters=MailQueryFilters(
                    date_from=datetime.now() - timedelta(days=days_back)
                ),
                pagination=PaginationOptions(top=max_mails, skip=0, max_pages=1),
                select_fields=[
                    "id",
                    "subject",
                    "from",
                    "sender",
                    "receivedDateTime",
                    "bodyPreview",
                    "body",
                    "hasAttachments",
                    "importance",
                    "isRead",
                ],
            )

            async with self.mail_query as orchestrator:
                query_response = await orchestrator.mail_query_user_emails(request)

            result["query_success"] = True
            result["total_mails_found"] = query_response.total_fetched
            result["execution_time"]["query_ms"] = query_response.execution_time_ms

            logger.info(
                f"âœ… [{user_id}] ë©”ì¼ ì¡°íšŒ ì™„ë£Œ: {query_response.total_fetched}ê°œ"
            )

            if query_response.total_fetched == 0:
                logger.info(f"âš ï¸ [{user_id}] ì¡°íšŒëœ ë©”ì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                result["execution_time"]["total_ms"] = int(
                    (datetime.now() - start_time).total_seconds() * 1000
                )
                return result

            # 2. Mail Process - ë©”ì¼ ì²˜ë¦¬
            logger.info(f"ğŸ”§ [{user_id}] ë©”ì¼ ì²˜ë¦¬ ì‹œì‘...")
            logger.info(
                f"   - ì¤‘ë³µ ì²´í¬: {'ON' if self.duplicate_check_enabled else 'OFF'}"
            )
            logger.info(
                f"   - êµ¬ì¡°í™” ì¶”ì¶œ: {'ON' if self.structured_extraction else 'OFF'}"
            )
            logger.info(f"   - ë°°ì¹˜ ì²˜ë¦¬: {'ON' if self.batch_extraction else 'OFF'}")

            process_start = datetime.now()

            # ë©”ì¼ ì²˜ë¦¬
            process_stats = await self.mail_processor.process_mails(
                account_id=user_id,
                mails=[mail.model_dump() for mail in query_response.messages],
                publish_batch_event=False,  # í…ŒìŠ¤íŠ¸ì—ì„œëŠ” ë°°ì¹˜ ì´ë²¤íŠ¸ ë°œí–‰ ì•ˆí•¨
            )

            result["process_success"] = True

            # í†µê³„ ë§¤í•‘
            result["mails_processed"] = process_stats.get("total_mails", 0)
            result["processing_stats"] = {
                "success": process_stats.get("saved_mails", 0),
                "skipped": process_stats.get("skipped_mails", 0),
                "failed": process_stats.get("db_errors", 0),
                "filtered": process_stats.get("filtered_mails", 0),
                "duplicate": process_stats.get("duplicate_mails", 0),
                "processed": process_stats.get("processed_mails", 0),
                "events_published": process_stats.get("events_published", 0),
                "keywords_extracted": process_stats.get("keywords_extracted", 0),
            }

            # í•„í„° ì´ìœ  ìƒì„¸
            if "filter_reasons" in process_stats:
                result["filter_details"] = {
                    "total": process_stats.get("skipped_mails", 0),
                    "reasons": process_stats["filter_reasons"],
                }

            # í‚¤ì›Œë“œ ìˆ˜ì§‘ ë° ë¶„ì„
            if "keywords" in process_stats and process_stats["keywords"]:
                all_keywords = process_stats["keywords"]
                result["keywords"]["all"] = all_keywords
                result["keywords"]["unique"] = list(set(all_keywords))

                # ìƒìœ„ 10ê°œ í‚¤ì›Œë“œ
                keyword_counter = Counter(all_keywords)
                result["keywords"]["top_10"] = [
                    {"keyword": k, "count": v}
                    for k, v in keyword_counter.most_common(10)
                ]

            # êµ¬ì¡°í™”ëœ ë°ì´í„° ìƒ˜í”Œ (ìˆëŠ” ê²½ìš°)
            if "structured_data_samples" in process_stats:
                result["structured_data_samples"] = process_stats[
                    "structured_data_samples"
                ][
                    :3
                ]  # ìµœëŒ€ 3ê°œ

            result["execution_time"]["process_ms"] = int(
                (datetime.now() - process_start).total_seconds() * 1000
            )

            # ì²˜ë¦¬ ê²°ê³¼ ë¡œê·¸
            self._log_process_result(user_id, process_stats)

        except Exception as e:
            error_msg = f"ê³„ì • ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}"
            logger.error(f"âŒ [{user_id}] {error_msg}", exc_info=True)
            result["errors"].append(error_msg)

        finally:
            result["execution_time"]["total_ms"] = int(
                (datetime.now() - start_time).total_seconds() * 1000
            )

        return result

    def _log_process_result(self, user_id: str, process_stats: Dict[str, Any]):
        """ì²˜ë¦¬ ê²°ê³¼ ë¡œê¹…"""
        if self.duplicate_check_enabled:
            logger.info(
                f"âœ… [{user_id}] ë©”ì¼ ì²˜ë¦¬ ì™„ë£Œ (ì¤‘ë³µ ì²´í¬ ON): "
                f"ì €ì¥={process_stats.get('saved_mails', 0)}, "
                f"ì¤‘ë³µ={process_stats.get('duplicate_mails', 0)}, "
                f"í•„í„°ë§={process_stats.get('skipped_mails', 0)}, "
                f"ì´ë²¤íŠ¸={process_stats.get('events_published', 0)}, "
                f"í‚¤ì›Œë“œ={process_stats.get('keywords_extracted', 0)}"
            )
        else:
            logger.info(
                f"âœ… [{user_id}] ë©”ì¼ ì²˜ë¦¬ ì™„ë£Œ (ì¤‘ë³µ ì²´í¬ OFF): "
                f"ì´ë²¤íŠ¸ ë°œí–‰={process_stats.get('events_published', 0)}ê°œ, "
                f"í‚¤ì›Œë“œ={process_stats.get('keywords_extracted', 0)}ê°œ "
                f"(DB ì €ì¥ ì—†ì´ ëª¨ë“  ë©”ì¼ì— ëŒ€í•´ ì´ë²¤íŠ¸ ë°œí–‰)"
            )

    async def test_all_accounts(
        self,
        days_back: int = 60,
        max_mails_per_account: int = 20,
        save_results: bool = True,
    ):
        """ëª¨ë“  ê³„ì • í†µí•© í…ŒìŠ¤íŠ¸"""

        print("\n" + "ğŸš€ " * 20)
        print("ëª¨ë“  ê³„ì • ë©”ì¼ ì¡°íšŒ ë° ì²˜ë¦¬ í†µí•© í…ŒìŠ¤íŠ¸")
        print("ğŸš€ " * 20)
        print(f"\nğŸ“… ì„¤ì •:")
        print(f"  - ì¡°íšŒ ê¸°ê°„: ìµœê·¼ {days_back}ì¼")
        print(f"  - ê³„ì •ë‹¹ ìµœëŒ€ ë©”ì¼: {max_mails_per_account}ê°œ")
        print(
            f"  - ì¤‘ë³µ ì²´í¬: {'í™œì„±í™”' if self.duplicate_check_enabled else 'ë¹„í™œì„±í™”'}"
        )
        print(
            f"  - êµ¬ì¡°í™” ì¶”ì¶œ: {'í™œì„±í™”' if self.structured_extraction else 'ë¹„í™œì„±í™”'}"
        )
        print(f"  - ë°°ì¹˜ ì²˜ë¦¬: {'í™œì„±í™”' if self.batch_extraction else 'ë¹„í™œì„±í™”'}")
        print(f"  - ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        if not self.duplicate_check_enabled:
            print("\nâš ï¸  ì£¼ì˜: ì¤‘ë³µ ì²´í¬ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤!")
            print("  - DB ì €ì¥ ì—†ì´ ëª¨ë“  ë©”ì¼ì— ëŒ€í•´ ì´ë²¤íŠ¸ê°€ ë°œí–‰ë©ë‹ˆë‹¤.")
            print("  - ë™ì¼í•œ ë©”ì¼ì„ ì—¬ëŸ¬ ë²ˆ ì²˜ë¦¬í•´ë„ ë§¤ë²ˆ ì´ë²¤íŠ¸ê°€ ë°œí–‰ë©ë‹ˆë‹¤.")

        print("\n" + "=" * 80)

        # 1. í™œì„± ê³„ì • ì¡°íšŒ
        accounts = await self.get_all_active_accounts()
        if not accounts:
            print("âŒ í™œì„± ê³„ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        print(f"\nğŸ“‹ í™œì„± ê³„ì •: {len(accounts)}ê°œ (í…ŒìŠ¤íŠ¸ ê³„ì • ì œì™¸)")
        for i, account in enumerate(accounts, 1):
            print(f"  {i}. {account['user_id']} ({account['user_name']})")

        # 2. ê° ê³„ì • ì²˜ë¦¬
        print("\n" + "=" * 80)
        print("ğŸ“§ ê³„ì •ë³„ ì²˜ë¦¬ ì‹œì‘")
        print("=" * 80)

        all_results = []
        total_stats = {
            "accounts": len(accounts),
            "successful_accounts": 0,
            "total_mails_found": 0,
            "total_mails_processed": 0,
            "total_saved": 0,
            "total_duplicate": 0,
            "total_filtered": 0,
            "total_events": 0,
            "total_keywords_extracted": 0,
            "all_keywords": [],
            "duplicate_check_enabled": self.duplicate_check_enabled,
            "structured_extraction": self.structured_extraction,
            "batch_extraction": self.batch_extraction,
        }

        for i, account in enumerate(accounts, 1):
            print(f"\n[{i}/{len(accounts)}] ì²˜ë¦¬ ì¤‘: {account['user_id']}")
            print("-" * 60)

            result = await self.process_account(
                user_id=account["user_id"],
                user_name=account["user_name"],
                days_back=days_back,
                max_mails=max_mails_per_account,
            )

            all_results.append(result)

            # í†µê³„ ì—…ë°ì´íŠ¸
            if result["query_success"] and result["process_success"]:
                total_stats["successful_accounts"] += 1

            total_stats["total_mails_found"] += result["total_mails_found"]
            total_stats["total_mails_processed"] += result["mails_processed"]
            total_stats["total_saved"] += result["processing_stats"]["success"]
            total_stats["total_duplicate"] += result["processing_stats"]["duplicate"]
            total_stats["total_filtered"] += result["processing_stats"]["skipped"]
            total_stats["total_events"] += result["processing_stats"][
                "events_published"
            ]
            total_stats["total_keywords_extracted"] += result["processing_stats"][
                "keywords_extracted"
            ]
            total_stats["all_keywords"].extend(result["keywords"]["all"])

            # ìƒì„¸ ê²°ê³¼ ì¶œë ¥
            self._print_account_result(result)

        # 3. ì „ì²´ í†µê³„
        self._print_total_stats(total_stats, all_results)

        # 4. ê²°ê³¼ ì €ì¥
        if save_results:
            filename = self._save_results(
                total_stats, all_results, days_back, max_mails_per_account
            )
            print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {filename}")

        print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print(f"ì¢…ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 80)

        return {"summary": total_stats, "results": all_results}

    def _print_account_result(self, result: Dict[str, Any]):
        """ê³„ì •ë³„ ê²°ê³¼ ì¶œë ¥"""
        print(f"  ğŸ“Š ê²°ê³¼:")
        if self.duplicate_check_enabled:
            print(f"     - ê¸°ì¡´ ì €ì¥ ë©”ì¼: ì „ì²´={result['existing_mails']['total']}ê°œ")
        print(f"     - ì¡°íšŒëœ ë©”ì¼: {result['total_mails_found']}ê°œ")

        if self.duplicate_check_enabled:
            print(
                f"     - ì²˜ë¦¬ ê²°ê³¼: ì €ì¥={result['processing_stats']['success']}, "
                f"ì¤‘ë³µ={result['processing_stats']['duplicate']}, "
                f"í•„í„°ë§={result['processing_stats']['skipped']}"
            )
        else:
            print(f"     - ì²˜ë¦¬ ê²°ê³¼: í•„í„°ë§={result['processing_stats']['skipped']}")

        print(f"     - ì´ë²¤íŠ¸ ë°œí–‰: {result['processing_stats']['events_published']}ê°œ")
        print(
            f"     - í‚¤ì›Œë“œ ì¶”ì¶œ: {result['processing_stats']['keywords_extracted']}ê°œ"
        )

        # í‚¤ì›Œë“œ ìƒìœ„ 5ê°œ í‘œì‹œ
        if result["keywords"]["top_10"]:
            print(f"     - ìƒìœ„ í‚¤ì›Œë“œ: ", end="")
            top_5 = result["keywords"]["top_10"][:5]
            keywords_str = ", ".join([f"{k['keyword']}({k['count']})" for k in top_5])
            print(keywords_str)

        # í•„í„°ë§ ìƒì„¸
        if result.get("filter_details") and result["filter_details"].get("reasons"):
            print(f"     - í•„í„°ë§ ìƒì„¸:")
            for reason, count in result["filter_details"]["reasons"].items():
                print(f"       â€¢ {reason}: {count}ê°œ")

        print(
            f"     - ì‹¤í–‰ ì‹œê°„: ì¡°íšŒ={result['execution_time']['query_ms']}ms, "
            f"ì²˜ë¦¬={result['execution_time']['process_ms']}ms"
        )

    def _print_total_stats(
        self, total_stats: Dict[str, Any], all_results: List[Dict[str, Any]]
    ):
        """ì „ì²´ í†µê³„ ì¶œë ¥"""
        print("\n" + "=" * 80)
        print("ğŸ“Š ì „ì²´ í†µê³„")
        print("=" * 80)

        print(f"\nâœ… ê³„ì • ì²˜ë¦¬ ê²°ê³¼:")
        print(f"  - ì „ì²´ ê³„ì •: {total_stats['accounts']}ê°œ")
        print(f"  - ì„±ê³µ ê³„ì •: {total_stats['successful_accounts']}ê°œ")
        print(
            f"  - ì‹¤íŒ¨ ê³„ì •: {total_stats['accounts'] - total_stats['successful_accounts']}ê°œ"
        )

        print(f"\nğŸ“§ ë©”ì¼ ì²˜ë¦¬ í†µê³„:")
        print(f"  - ì¡°íšŒëœ ì´ ë©”ì¼: {total_stats['total_mails_found']}ê°œ")
        print(f"  - ì²˜ë¦¬ ì‹œë„: {total_stats['total_mails_processed']}ê°œ")

        if self.duplicate_check_enabled:
            print(f"  - ì €ì¥ëœ ë©”ì¼: {total_stats['total_saved']}ê°œ")
            print(f"  - ì¤‘ë³µ ë©”ì¼: {total_stats['total_duplicate']}ê°œ")
            print(f"  - í•„í„°ë§ëœ ë©”ì¼: {total_stats['total_filtered']}ê°œ")

            # ì„±ê³µë¥  ê³„ì‚°
            if total_stats["total_mails_processed"] > 0:
                save_rate = (
                    total_stats["total_saved"] / total_stats["total_mails_processed"]
                ) * 100
                print(f"  - ì €ì¥ë¥ : {save_rate:.1f}%")
        else:
            print(f"  - í•„í„°ë§ëœ ë©”ì¼: {total_stats['total_filtered']}ê°œ")
            print(f"  - DB ì €ì¥: 0ê°œ (ì¤‘ë³µ ì²´í¬ OFF)")

        print(f"  - ë°œí–‰ëœ ì´ë²¤íŠ¸: {total_stats['total_events']}ê°œ")
        print(f"  - ì¶”ì¶œëœ í‚¤ì›Œë“œ: {total_stats['total_keywords_extracted']}ê°œ")

        # í‚¤ì›Œë“œ ë¶„ì„
        if total_stats["all_keywords"]:
            unique_keywords = list(set(total_stats["all_keywords"]))
            keyword_counter = Counter(total_stats["all_keywords"])
            top_20_keywords = keyword_counter.most_common(20)

            print(f"\nğŸ”‘ í‚¤ì›Œë“œ ë¶„ì„:")
            print(f"  - ì´ í‚¤ì›Œë“œ ìˆ˜: {len(total_stats['all_keywords'])}ê°œ")
            print(f"  - ê³ ìœ  í‚¤ì›Œë“œ ìˆ˜: {len(unique_keywords)}ê°œ")
            print(f"  - ìƒìœ„ 20ê°œ í‚¤ì›Œë“œ:")
            for i, (keyword, count) in enumerate(top_20_keywords, 1):
                print(f"    {i:2d}. {keyword}: {count}íšŒ")

        # ì‹¤í–‰ ì‹œê°„ ë¶„ì„
        total_query_time = sum(r["execution_time"]["query_ms"] for r in all_results)
        total_process_time = sum(r["execution_time"]["process_ms"] for r in all_results)
        total_time = sum(r["execution_time"]["total_ms"] for r in all_results)

        print(f"\nâ±ï¸  ì‹¤í–‰ ì‹œê°„ ë¶„ì„:")
        print(
            f"  - ì´ ì¡°íšŒ ì‹œê°„: {total_query_time:,}ms ({total_query_time/1000:.1f}ì´ˆ)"
        )
        print(
            f"  - ì´ ì²˜ë¦¬ ì‹œê°„: {total_process_time:,}ms ({total_process_time/1000:.1f}ì´ˆ)"
        )
        print(f"  - ì´ ì‹¤í–‰ ì‹œê°„: {total_time:,}ms ({total_time/1000:.1f}ì´ˆ)")
        print(f"  - í‰ê·  ì‹œê°„/ê³„ì •: {total_time/len(all_results):.0f}ms")

        # ìƒì„¸ ê²°ê³¼ í…Œì´ë¸”
        self._print_result_table(all_results)

    def _print_result_table(self, all_results: List[Dict[str, Any]]):
        """ê²°ê³¼ í…Œì´ë¸” ì¶œë ¥"""
        print(f"\nğŸ“‹ ê³„ì •ë³„ ìƒì„¸ ê²°ê³¼:")

        if self.duplicate_check_enabled:
            print(
                f"{'ê³„ì •':<15} {'ê¸°ì¡´':<8} {'ì¡°íšŒ':<8} {'ì €ì¥':<8} {'ì¤‘ë³µ':<8} {'í•„í„°ë§':<8} {'ì´ë²¤íŠ¸':<8} {'í‚¤ì›Œë“œ':<8} {'ì‹œê°„(ì´ˆ)':<10}"
            )
            print("-" * 101)

            for result in all_results:
                print(
                    f"{result['user_id']:<15} "
                    f"{result['existing_mails']['total']:<8} "
                    f"{result['total_mails_found']:<8} "
                    f"{result['processing_stats']['success']:<8} "
                    f"{result['processing_stats']['duplicate']:<8} "
                    f"{result['processing_stats']['skipped']:<8} "
                    f"{result['processing_stats']['events_published']:<8} "
                    f"{result['processing_stats']['keywords_extracted']:<8} "
                    f"{result['execution_time']['total_ms']/1000:<10.1f}"
                )
        else:
            print(
                f"{'ê³„ì •':<15} {'ì¡°íšŒ':<8} {'í•„í„°ë§':<8} {'ì´ë²¤íŠ¸':<8} {'í‚¤ì›Œë“œ':<8} {'ì‹œê°„(ì´ˆ)':<10}"
            )
            print("-" * 68)

            for result in all_results:
                print(
                    f"{result['user_id']:<15} "
                    f"{result['total_mails_found']:<8} "
                    f"{result['processing_stats']['skipped']:<8} "
                    f"{result['processing_stats']['events_published']:<8} "
                    f"{result['processing_stats']['keywords_extracted']:<8} "
                    f"{result['execution_time']['total_ms']/1000:<10.1f}"
                )

    def _save_results(
        self,
        total_stats: Dict[str, Any],
        all_results: List[Dict[str, Any]],
        days_back: int,
        max_mails_per_account: int,
    ) -> str:
        """ê²°ê³¼ ì €ì¥"""
        filename = (
            f"mail_process_test_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        )
        if not self.duplicate_check_enabled:
            filename += "_dup_check_off"
        if self.structured_extraction:
            filename += "_structured"
        if self.batch_extraction:
            filename += "_batch"
        filename += ".json"

        save_data = {
            "test_info": {
                "test_date": datetime.now().isoformat(),
                "days_back": days_back,
                "max_mails_per_account": max_mails_per_account,
                "duplicate_check_enabled": self.duplicate_check_enabled,
                "structured_extraction": self.structured_extraction,
                "batch_extraction": self.batch_extraction,
            },
            "summary": total_stats,
            "detailed_results": all_results,
        }

        with open(filename, "w", encoding="utf-8") as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2, default=str)

        return filename

    async def test_duplicate_check_behavior(
        self, user_id: str, test_mail_count: int = 5
    ):
        """íŠ¹ì • ê³„ì •ì— ëŒ€í•´ ì¤‘ë³µ ì²´í¬ ë™ì‘ í…ŒìŠ¤íŠ¸"""
        print(f"\nğŸ§ª ì¤‘ë³µ ì²´í¬ ë™ì‘ í…ŒìŠ¤íŠ¸: {user_id}")
        print("=" * 60)

        # 1ì°¨ ì‹¤í–‰ (ì²˜ìŒ ì‹¤í–‰)
        print("\n1ï¸âƒ£ ì²« ë²ˆì§¸ ì‹¤í–‰:")
        result1 = await self.process_account(
            user_id=user_id, user_name=user_id, days_back=7, max_mails=test_mail_count
        )

        print(f"  - ì¡°íšŒ: {result1['total_mails_found']}ê°œ")
        print(f"  - ì €ì¥: {result1['processing_stats']['success']}ê°œ")
        print(f"  - ì´ë²¤íŠ¸: {result1['processing_stats']['events_published']}ê°œ")
        print(f"  - í‚¤ì›Œë“œ: {result1['processing_stats']['keywords_extracted']}ê°œ")

        # ì ì‹œ ëŒ€ê¸°
        await asyncio.sleep(2)

        # 2ì°¨ ì‹¤í–‰ (ë™ì¼í•œ ë©”ì¼ ì¬ì²˜ë¦¬)
        print("\n2ï¸âƒ£ ë‘ ë²ˆì§¸ ì‹¤í–‰ (ë™ì¼ ë©”ì¼):")
        result2 = await self.process_account(
            user_id=user_id, user_name=user_id, days_back=7, max_mails=test_mail_count
        )

        print(f"  - ì¡°íšŒ: {result2['total_mails_found']}ê°œ")

        if self.duplicate_check_enabled:
            print(f"  - ì €ì¥: {result2['processing_stats']['success']}ê°œ (ì˜ˆìƒ: 0)")
            print(
                f"  - ì¤‘ë³µ: {result2['processing_stats']['duplicate']}ê°œ (ì˜ˆìƒ: {result1['processing_stats']['success']})"
            )
            print(
                f"  - ì´ë²¤íŠ¸: {result2['processing_stats']['events_published']}ê°œ (ì˜ˆìƒ: 0)"
            )

            # ê²€ì¦
            if (
                result2["processing_stats"]["duplicate"]
                == result1["processing_stats"]["success"]
            ):
                print("\nâœ… ì¤‘ë³µ ì²´í¬ ì •ìƒ ë™ì‘: ëª¨ë“  ë©”ì¼ì´ ì¤‘ë³µìœ¼ë¡œ ì²˜ë¦¬ë¨")
            else:
                print("\nâŒ ì¤‘ë³µ ì²´í¬ ì˜¤ë¥˜: ì¼ë¶€ ë©”ì¼ì´ ì¤‘ë³µìœ¼ë¡œ ì²˜ë¦¬ë˜ì§€ ì•ŠìŒ")
        else:
            print(
                f"  - ì´ë²¤íŠ¸: {result2['processing_stats']['events_published']}ê°œ (ì˜ˆìƒ: {result2['total_mails_found'] - result2['processing_stats']['skipped']})"
            )
            print("\nâœ… ì¤‘ë³µ ì²´í¬ OFF: DB í™•ì¸ ì—†ì´ ëª¨ë“  ë©”ì¼ì— ëŒ€í•´ ì´ë²¤íŠ¸ ë°œí–‰")

        return {
            "first_run": result1,
            "second_run": result2,
            "duplicate_check_working": (
                result2["processing_stats"]["duplicate"]
                == result1["processing_stats"]["success"]
                if self.duplicate_check_enabled
                else True
            ),
        }

    async def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            await self.mail_query.close()
        except Exception as e:
            logger.debug(f"mail_query ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {e}")

        try:
            await self.mail_processor.close()
        except Exception as e:
            logger.debug(f"mail_processor ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {e}")


def clear_mail_history():
    """mail_history í…Œì´ë¸” ì´ˆê¸°í™”"""
    config = get_config()
    db_path = config.database_path

    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()

        # í˜„ì¬ ë ˆì½”ë“œ ìˆ˜ í™•ì¸
        cursor.execute("SELECT COUNT(*) FROM mail_history;")
        count = cursor.fetchone()[0]

        if count > 0:
            print(f"ğŸ—‘ï¸  {count}ê°œì˜ ë ˆì½”ë“œë¥¼ ì‚­ì œí•©ë‹ˆë‹¤...")
            cursor.execute("DELETE FROM mail_history;")
            conn.commit()
            print(f"âœ… mail_history í…Œì´ë¸”ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print(f"â„¹ï¸  mail_history í…Œì´ë¸”ì´ ì´ë¯¸ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")

        conn.close()
    except Exception as e:
        print(f"âŒ mail_history í…Œì´ë¸” ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import sys

    # ëª…ë ¹í–‰ ì¸ìˆ˜ ì²˜ë¦¬
    days_back = 60
    max_mails = 20
    test_duplicate = False
    test_user = None
    clear_history = False

    if len(sys.argv) > 1:
        if sys.argv[1] == "--help":
            print("ì‚¬ìš©ë²•: python mail_query_process.py [days] [max_mails] [options]")
            print("\nìœ„ì¹˜ ì¸ìˆ˜:")
            print("  days         ì¡°íšŒí•  ê³¼ê±° ì¼ìˆ˜ (ê¸°ë³¸: 60)")
            print("  max_mails    ê³„ì •ë‹¹ ìµœëŒ€ ë©”ì¼ ìˆ˜ (ê¸°ë³¸: 20)")
            print("\nì˜µì…˜:")
            print("  --test-duplicate USER_ID    íŠ¹ì • ì‚¬ìš©ìì— ëŒ€í•´ ì¤‘ë³µ ì²´í¬ í…ŒìŠ¤íŠ¸")
            print("  --clear-history            í…ŒìŠ¤íŠ¸ ì „ mail_history í…Œì´ë¸” ì´ˆê¸°í™”")
            print("  --help                     ì´ ë„ì›€ë§ í‘œì‹œ")
            print("\ní™˜ê²½ ë³€ìˆ˜:")
            print("  ENABLE_MAIL_DUPLICATE_CHECK      ì¤‘ë³µ ì²´í¬ í™œì„±í™” (ê¸°ë³¸: true)")
            print(
                "  ENABLE_STRUCTURED_EXTRACTION     êµ¬ì¡°í™”ëœ ì¶”ì¶œ í™œì„±í™” (ê¸°ë³¸: true)"
            )
            print("  ENABLE_BATCH_KEYWORD_EXTRACTION  ë°°ì¹˜ ì¶”ì¶œ í™œì„±í™” (ê¸°ë³¸: true)")
            return

        # ì˜µì…˜ íŒŒì‹±
        args = []
        i = 1
        while i < len(sys.argv):
            if sys.argv[i] == "--test-duplicate":
                test_duplicate = True
                if i + 1 < len(sys.argv):
                    test_user = sys.argv[i + 1]
                    i += 1
                else:
                    print("ì˜¤ë¥˜: --test-duplicate ì˜µì…˜ì—ëŠ” user_idê°€ í•„ìš”í•©ë‹ˆë‹¤")
                    return
            elif sys.argv[i] == "--clear-history":
                clear_history = True
            else:
                args.append(sys.argv[i])
            i += 1

        # ìœ„ì¹˜ ì¸ìˆ˜ ì²˜ë¦¬
        if len(args) > 0:
            days_back = int(args[0])
        if len(args) > 1:
            max_mails = int(args[1])

    # mail_history í…Œì´ë¸” ì´ˆê¸°í™” (ì˜µì…˜ìœ¼ë¡œ ì§€ì •ëœ ê²½ìš°)
    if clear_history:
        print("ğŸ—‘ï¸  mail_history í…Œì´ë¸” ì´ˆê¸°í™” ì¤‘...")
        clear_mail_history()
        print()

    tester = AllAccountsFullProcessTester()

    try:
        if test_duplicate and test_user:
            # ì¤‘ë³µ ì²´í¬ ë™ì‘ í…ŒìŠ¤íŠ¸
            await tester.test_duplicate_check_behavior(test_user, test_mail_count=5)
        else:
            # ì „ì²´ ê³„ì • í…ŒìŠ¤íŠ¸
            await tester.test_all_accounts(
                days_back=days_back, max_mails_per_account=max_mails, save_results=True
            )

    finally:
        await tester.close()


if __name__ == "__main__":
    asyncio.run(main())
