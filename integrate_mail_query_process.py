#!/usr/bin/env python3
"""
Mail Query + Mail Processor í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ìµœê·¼ ë©”ì¼ì„ ì¡°íšŒí•˜ê³  ì²˜ë¦¬í•˜ë©° Kafka ì´ë²¤íŠ¸ ë°œí–‰ê¹Œì§€ í™•ì¸
"""
import asyncio
import sys
import json
from datetime import datetime, timedelta
from typing import List, Dict, Any

# ëª¨ë“ˆ ì„í¬íŠ¸
from modules.mail_query import (
    MailQueryOrchestrator,
    MailQueryRequest,
    MailQueryFilters,
    PaginationOptions
)
from modules.mail_processor import (
    MailProcessorOrchestrator,
    GraphMailItem,
    ProcessingStatus
)
from infra.core.logger import get_logger
from infra.core.database import get_database_manager
from infra.core.kafka_client import get_kafka_client

logger = get_logger(__name__)


class MailIntegrationProcessor:
    """ë©”ì¼ ì¡°íšŒ ë° ì²˜ë¦¬ í†µí•© í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.mail_query_orchestrator = MailQueryOrchestrator()
        self.mail_processor_orchestrator = MailProcessorOrchestrator()
        self.db_manager = get_database_manager()
        self.kafka_client = get_kafka_client()
        
        # Kafka ì´ë²¤íŠ¸ ë°œí–‰ í†µê³„
        self.kafka_events_published = []
        self.kafka_publish_errors = []
    
    async def process_recent_mails(self, user_id: str = "kimghw", mail_count: int = 6) -> dict:
        """ìµœê·¼ ë©”ì¼ ì¡°íšŒ ë° ì²˜ë¦¬ í†µí•© ì›Œí¬í”Œë¡œìš°"""
        start_time = datetime.now()
        
        # í†µê³„ ì´ˆê¸°í™”
        self.kafka_events_published = []
        self.kafka_publish_errors = []
        
        try:
            logger.info(f"ğŸš€ ë©”ì¼ í†µí•© ì²˜ë¦¬ ì‹œì‘")
            logger.info(f"==================================================")
            logger.info(f"ğŸ”„ [CALL STACK] MailIntegrationProcessor.process_recent_mails() ì‹œì‘")
            logger.info(f"ğŸ“‹ [PARAMS] ì‚¬ìš©ì: {user_id}, ì¡°íšŒ ê°œìˆ˜: {mail_count}")
            logger.info(f"=== ë©”ì¼ í†µí•© ì²˜ë¦¬ ì‹œì‘ ===")
            
            # 1ë‹¨ê³„: Mail Queryë¡œ ìµœê·¼ ë©”ì¼ ì¡°íšŒ
            logger.info(f"ğŸ”„ [CALL STACK] â†’ 1ë‹¨ê³„: _query_recent_mails() í˜¸ì¶œ")
            logger.info("1ë‹¨ê³„: ìµœê·¼ ë©”ì¼ ì¡°íšŒ ì¤‘...")
            query_result = await self._query_recent_mails(user_id, mail_count)
            
            if not query_result['success']:
                return {
                    'success': False,
                    'error': query_result['error'],
                    'stage': 'mail_query'
                }
            
            messages = query_result['messages']
            logger.info(f"ì¡°íšŒ ì™„ë£Œ: {len(messages)}ê°œ ë©”ì¼")
            
            # 2ë‹¨ê³„: Mail Processorë¡œ ê° ë©”ì¼ ì²˜ë¦¬ + Kafka ì´ë²¤íŠ¸ ë°œí–‰
            logger.info("2ë‹¨ê³„: ë©”ì¼ ì²˜ë¦¬ ì¤‘...")
            logger.info(f"ğŸ”„ [CALL STACK] â†’ 2ë‹¨ê³„: _process_messages() ì‹¤í–‰")
            processing_results = await self._process_messages(user_id, messages)
            
            # 3ë‹¨ê³„: Kafka ì´ë²¤íŠ¸ ë°œí–‰ ìƒíƒœ í™•ì¸
            await self._verify_kafka_events()
            
            # 4ë‹¨ê³„: ê²°ê³¼ ì§‘ê³„
            execution_time = (datetime.now() - start_time).total_seconds()
            
            result = {
                'success': True,
                'user_id': user_id,
                'total_queried': len(messages),
                'processing_results': processing_results,
                'kafka_stats': {
                    'events_published': len(self.kafka_events_published),
                    'publish_errors': len(self.kafka_publish_errors),
                    'published_event_ids': [e['event_id'] for e in self.kafka_events_published]
                },
                'summary': self._create_summary(processing_results),
                'execution_time_seconds': round(execution_time, 2),
                'query_info': query_result.get('query_info', {}),
                'timestamp': start_time.isoformat()
            }
            
            logger.info(f"=== í†µí•© ì²˜ë¦¬ ì™„ë£Œ ===")
            logger.info(f"ì´ ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ")
            
            return result
            
        except Exception as e:
            logger.error(f"í†µí•© ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}", exc_info=True)
            return {
                'success': False,
                'error': str(e),
                'stage': 'integration',
                'execution_time_seconds': (datetime.now() - start_time).total_seconds()
            }
    
    async def _query_recent_mails(self, user_id: str, mail_count: int) -> dict:
        """Mail Query ëª¨ë“ˆë¡œ ìµœê·¼ ë©”ì¼ ì¡°íšŒ"""
        try:
            logger.info(f"ğŸ”„ [CALL STACK] â†’ â†’ _query_recent_mails() ì‹¤í–‰")
            logger.info(f"ğŸ“‹ [PARAMS] user_id={user_id}, mail_count={mail_count}")
            
            # ìµœê·¼ 7ì¼ê°„ì˜ ë©”ì¼ë§Œ ì¡°íšŒ (ì„±ëŠ¥ ìµœì í™”)
            date_from = datetime.now() - timedelta(days=7)
            logger.info(f"ğŸ“… [FILTER] ì¡°íšŒ ê¸°ê°„: {date_from.strftime('%Y-%m-%d')} ~ í˜„ì¬")
            
            # ë©”ì¼ ì¡°íšŒ ìš”ì²­ êµ¬ì„±
            logger.info(f"ğŸ”§ [CONFIG] MailQueryRequest êµ¬ì„± ì¤‘...")
            request = MailQueryRequest(
                user_id=user_id,
                filters=MailQueryFilters(
                    date_from=date_from
                ),
                pagination=PaginationOptions(
                    top=mail_count,  # ìš”ì²­í•œ ê°œìˆ˜ë§Œí¼ë§Œ ì¡°íšŒ
                    skip=0,
                    max_pages=1  # 1í˜ì´ì§€ë§Œ ì¡°íšŒ
                ),
                select_fields=[
                    "id", "subject", "from", "sender", "receivedDateTime", 
                    "bodyPreview", "body", "hasAttachments", "importance", "isRead"
                ]
            )
            
            # Mail Query ì‹¤í–‰
            logger.info(f"ğŸ”— [MODULE] MailQueryOrchestrator.mail_query_user_emails() í˜¸ì¶œ")
            async with self.mail_query_orchestrator as orchestrator:
                response = await orchestrator.mail_query_user_emails(request)
            
            logger.info(f"âœ… [SUCCESS] ë©”ì¼ ì¡°íšŒ ì„±ê³µ: {response.total_fetched}ê°œ")
            logger.info(f"â±ï¸ [PERF] Mail Query ì‹¤í–‰ì‹œê°„: {response.execution_time_ms}ms")
            
            return {
                'success': True,
                'messages': response.messages,
                'query_info': {
                    'total_fetched': response.total_fetched,
                    'execution_time_ms': response.execution_time_ms,
                    'has_more': response.has_more,
                    'performance_estimate': response.query_info.get('performance_estimate', 'UNKNOWN')
                }
            }
            
        except Exception as e:
            logger.error(f"ë©”ì¼ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'messages': []
            }
    
    async def _process_messages(self, user_id: str, messages: List[GraphMailItem]) -> List[dict]:
        """Mail Processor ëª¨ë“ˆë¡œ ê° ë©”ì¼ ì²˜ë¦¬"""
        logger.info(f"ğŸ“‹ [PARAMS] user_id={user_id}, messages_count={len(messages)}")
        processing_results = []
        
        # ì›ë˜ Kafka ë°œí–‰ í•¨ìˆ˜ë¥¼ ë˜í•‘í•˜ì—¬ ëª¨ë‹ˆí„°ë§
        original_publish = self.mail_processor_orchestrator.kafka_helper.publish_kafka_event
        
        async def monitored_publish(account_id: str, mail: Dict, keywords: List[str] = None):
            """Kafka ë°œí–‰ì„ ëª¨ë‹ˆí„°ë§í•˜ëŠ” ë˜í¼ í•¨ìˆ˜"""
            try:
                # ì›ë˜ í•¨ìˆ˜ í˜¸ì¶œ
                await original_publish(account_id, mail, keywords)
                
                # ë°œí–‰ ì„±ê³µ ê¸°ë¡
                event_info = {
                    'event_id': mail.get('id', 'unknown'),
                    'account_id': account_id,
                    'subject': mail.get('subject', ''),
                    'keywords': keywords or [],
                    'timestamp': datetime.now().isoformat()
                }
                self.kafka_events_published.append(event_info)
                logger.info(f"ğŸ“¤ [KAFKA] ì´ë²¤íŠ¸ ë°œí–‰ ì„±ê³µ: {event_info['event_id']}")
                
            except Exception as e:
                # ë°œí–‰ ì‹¤íŒ¨ ê¸°ë¡
                error_info = {
                    'event_id': mail.get('id', 'unknown'),
                    'error': str(e),
                    'timestamp': datetime.now().isoformat()
                }
                self.kafka_publish_errors.append(error_info)
                logger.error(f"âŒ [KAFKA] ì´ë²¤íŠ¸ ë°œí–‰ ì‹¤íŒ¨: {error_info['event_id']} - {str(e)}")
                raise
        
        # ëª¨ë‹ˆí„°ë§ í•¨ìˆ˜ë¡œ êµì²´
        self.mail_processor_orchestrator.kafka_helper.publish_kafka_event = monitored_publish
        
        try:
            for i, message in enumerate(messages, 1):
                try:
                    logger.info(f"ğŸ“§ [MAIL {i}/{len(messages)}] ì²˜ë¦¬ ì¤‘: {message.subject[:50]}...")
                    logger.info(f"ğŸ”— [MODULE] MailProcessorOrchestrator.process_graph_mail_item() í˜¸ì¶œ")
                    
                    # Mail Processorë¡œ ê°œë³„ ë©”ì¼ ì²˜ë¦¬
                    processed_result = await self.mail_processor_orchestrator.process_graph_mail_item(
                        mail_item=message,
                        account_id=user_id
                    )
                    
                    # ê²°ê³¼ ì •ë¦¬
                    result = {
                        'mail_id': message.id,
                        'subject': message.subject or 'No Subject',
                        'sender': self._extract_sender_address(message),
                        'received_time': message.received_date_time.isoformat(),
                        'processing_status': processed_result.processing_status.value,
                        'keywords': processed_result.keywords,
                        'error_message': processed_result.error_message
                    }
                    
                    processing_results.append(result)
                    
                    # ê°„ë‹¨í•œ ë¡œê·¸
                    status_emoji = {
                        ProcessingStatus.SUCCESS: "âœ…",
                        ProcessingStatus.SKIPPED: "â­ï¸",
                        ProcessingStatus.FAILED: "âŒ"
                    }
                    
                    emoji = status_emoji.get(processed_result.processing_status, "â“")
                    logger.info(f"{emoji} [RESULT] ë©”ì¼ {i}: {processed_result.processing_status.value}")
                    
                    if processed_result.keywords:
                        logger.info(f"ğŸ“Š [DATA] í‚¤ì›Œë“œ ì¶”ì¶œ: {len(processed_result.keywords)}ê°œ")
                        logger.info(f"ğŸ·ï¸ [KEYWORDS] {', '.join(processed_result.keywords)}")
                    
                    if processed_result.error_message:
                        logger.warning(f"âš ï¸ [ERROR] ì˜¤ë¥˜: {processed_result.error_message}")
                    
                except Exception as e:
                    logger.error(f"ë©”ì¼ {i} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                    processing_results.append({
                        'mail_id': message.id,
                        'subject': message.subject or 'No Subject',
                        'sender': self._extract_sender_address(message),
                        'received_time': message.received_date_time.isoformat(),
                        'processing_status': 'ERROR',
                        'keywords': [],
                        'error_message': str(e)
                    })
        
        finally:
            # ì›ë˜ í•¨ìˆ˜ë¡œ ë³µì›
            self.mail_processor_orchestrator.kafka_helper.publish_kafka_event = original_publish
        
        return processing_results
    
    async def _verify_kafka_events(self):
        """Kafka ì´ë²¤íŠ¸ ë°œí–‰ ìƒíƒœ ê²€ì¦"""
        logger.info("ğŸ” [KAFKA] ì´ë²¤íŠ¸ ë°œí–‰ ìƒíƒœ ê²€ì¦ ì¤‘...")
        
        if self.kafka_events_published:
            logger.info(f"âœ… [KAFKA] ì´ {len(self.kafka_events_published)}ê°œ ì´ë²¤íŠ¸ ë°œí–‰ ì„±ê³µ")
            for event in self.kafka_events_published[:3]:  # ì²˜ìŒ 3ê°œë§Œ í‘œì‹œ
                logger.info(f"   - {event['event_id']}: {len(event['keywords'])}ê°œ í‚¤ì›Œë“œ")
        
        if self.kafka_publish_errors:
            logger.warning(f"âŒ [KAFKA] ì´ {len(self.kafka_publish_errors)}ê°œ ì´ë²¤íŠ¸ ë°œí–‰ ì‹¤íŒ¨")
            for error in self.kafka_publish_errors:
                logger.warning(f"   - {error['event_id']}: {error['error']}")
    
    def _extract_sender_address(self, message: GraphMailItem) -> str:
        """ë©”ì‹œì§€ì—ì„œ ë°œì‹ ì ì£¼ì†Œ ì¶”ì¶œ"""
        try:
            # from_address í•„ë“œ í™•ì¸
            if message.from_address and isinstance(message.from_address, dict):
                email_addr = message.from_address.get('emailAddress', {})
                if email_addr and email_addr.get('address'):
                    return email_addr['address']
            
            # sender í•„ë“œ í™•ì¸
            if message.sender and isinstance(message.sender, dict):
                email_addr = message.sender.get('emailAddress', {})
                if email_addr and email_addr.get('address'):
                    return email_addr['address']
            
            return 'Unknown Sender'
            
        except Exception:
            return 'Unknown Sender'
    
    def _create_summary(self, processing_results: List[dict]) -> dict:
        """ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ìƒì„±"""
        total = len(processing_results)
        success_count = sum(1 for r in processing_results if r['processing_status'] == 'SUCCESS')
        skipped_count = sum(1 for r in processing_results if r['processing_status'] == 'SKIPPED')
        failed_count = sum(1 for r in processing_results if r['processing_status'] in ['FAILED', 'ERROR'])
        
        # í‚¤ì›Œë“œ í†µê³„
        all_keywords = []
        for result in processing_results:
            all_keywords.extend(result.get('keywords', []))
        
        # í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚°
        from collections import Counter
        keyword_counter = Counter(all_keywords)
        top_keywords = [kw for kw, _ in keyword_counter.most_common(10)]
        
        return {
            'total_mails': total,
            'success_count': success_count,
            'skipped_count': skipped_count,
            'failed_count': failed_count,
            'success_rate': round((success_count / total * 100) if total > 0 else 0, 1),
            'total_keywords_extracted': len(all_keywords),
            'unique_keywords_count': len(set(all_keywords)),
            'top_keywords': top_keywords,
            'kafka_events_published': len(self.kafka_events_published),
            'kafka_publish_errors': len(self.kafka_publish_errors)
        }
    
    async def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            await self.mail_query_orchestrator.close()
            # Kafka flush
            self.kafka_client.flush()
        except Exception as e:
            logger.error(f"ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {str(e)}")


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ Mail Integration Test ì‹œì‘")
    print("=" * 50)
    
    # ì‚¬ìš©ì ì„¤ì •
    user_id = "kimghw"  # ì‹¤ì œ ì‚¬ìš©ì ID
    mail_count = 6      # ì¡°íšŒí•  ë©”ì¼ ê°œìˆ˜
    
    processor = MailIntegrationProcessor()
    
    try:
        # ëª…ë ¹í–‰ ì¸ìˆ˜ í™•ì¸
        if len(sys.argv) > 1:
            if sys.argv[1] == "--clear-data":
                print("ğŸ—‘ï¸  ë°ì´í„° ì´ˆê¸°í™” ëª¨ë“œ")
                print("=" * 50)
                
                # í…Œì´ë¸” ì •ë¦¬
                mail_history_result = processor.db_manager.clear_table_data("mail_history")
                print(f"ğŸ“§ mail_history: {mail_history_result['message']}")
                
                logs_result = processor.db_manager.clear_table_data("processing_logs")
                print(f"ğŸ“ processing_logs: {logs_result['message']}")
                
                print("\nâœ… ë°ì´í„° ì´ˆê¸°í™” ì™„ë£Œ")
                return
            
            elif sys.argv[1] == "--check-kafka":
                print("ğŸ” Kafka í† í”½ í™•ì¸ ëª¨ë“œ")
                print("=" * 50)
                
                # Kafka í† í”½ ë©”íƒ€ë°ì´í„° í™•ì¸
                try:
                    metadata = processor.kafka_client.get_topic_metadata('email-raw-data')
                    print(f"âœ… Kafka í† í”½ ìƒíƒœ: {metadata}")
                    
                    # ì—°ê²° ìƒíƒœ í™•ì¸
                    if processor.kafka_client.health_check():
                        print("âœ… Kafka ì—°ê²° ìƒíƒœ: ì •ìƒ")
                    else:
                        print("âŒ Kafka ì—°ê²° ìƒíƒœ: ë¹„ì •ìƒ")
                except Exception as e:
                    print(f"âŒ Kafka í™•ì¸ ì‹¤íŒ¨: {str(e)}")
                return
        
        # í†µí•© ì²˜ë¦¬ ì‹¤í–‰
        result = await processor.process_recent_mails(user_id, mail_count)
        
        # ê²°ê³¼ ì¶œë ¥
        print("\nğŸ“Š ì²˜ë¦¬ ê²°ê³¼:")
        print("=" * 50)
        
        if result['success']:
            summary = result['summary']
            print(f"âœ… ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ")
            print(f"ğŸ“§ ì´ ë©”ì¼: {summary['total_mails']}ê°œ")
            print(f"âœ… ì„±ê³µ: {summary['success_count']}ê°œ")
            print(f"â­ï¸  ê±´ë„ˆëœ€: {summary['skipped_count']}ê°œ")
            print(f"âŒ ì‹¤íŒ¨: {summary['failed_count']}ê°œ")
            print(f"ğŸ“ˆ ì„±ê³µë¥ : {summary['success_rate']}%")
            print(f"ğŸ”‘ ì¶”ì¶œëœ í‚¤ì›Œë“œ: {summary['total_keywords_extracted']}ê°œ")
            print(f"ğŸ“¤ Kafka ì´ë²¤íŠ¸ ë°œí–‰: {summary['kafka_events_published']}ê°œ")
            if summary['kafka_publish_errors'] > 0:
                print(f"âŒ Kafka ë°œí–‰ ì‹¤íŒ¨: {summary['kafka_publish_errors']}ê°œ")
            print(f"â±ï¸  ì‹¤í–‰ ì‹œê°„: {result['execution_time_seconds']}ì´ˆ")
            
            if summary['top_keywords']:
                print(f"\nğŸ·ï¸  ì£¼ìš” í‚¤ì›Œë“œ: {', '.join(summary['top_keywords'])}")
            
            # Kafka ì´ë²¤íŠ¸ ì •ë³´
            if result['kafka_stats']['events_published'] > 0:
                print(f"\nğŸ“¤ Kafka ì´ë²¤íŠ¸ ID:")
                for event_id in result['kafka_stats']['published_event_ids'][:5]:
                    print(f"   - {event_id}")
            
            # ì²˜ë¦¬ ìƒíƒœë³„ ì¹´ìš´íŠ¸
            print(f"\nğŸ“‹ ì²˜ë¦¬ ìƒíƒœ:")
            status_counts = {}
            for mail_result in result['processing_results']:
                status = mail_result['processing_status']
                status_counts[status] = status_counts.get(status, 0) + 1
            
            for status, count in status_counts.items():
                print(f"  {status}: {count}ê°œ")
        
        else:
            print(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {result['error']}")
            print(f"ğŸ“ ì‹¤íŒ¨ ë‹¨ê³„: {result.get('stage', 'unknown')}")
    
    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
    except Exception as e:
        print(f"\nğŸ’¥ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()
    
    finally:
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        await processor.close()
        print("\nğŸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")


if __name__ == "__main__":
    # ë¹„ë™ê¸° ì‹¤í–‰
    asyncio.run(main())